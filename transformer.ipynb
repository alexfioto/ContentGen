{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN8QGb6yrgq8NQpDzX1MN+X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7yEFBhfzzop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44fbfeef-f6f0-4928-a70c-14efc6bae144"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMw9mUyS9_rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code inspired and adapted from https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnxTaijq-V97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://arxiv.org/abs/1706.03762\n",
        "\n",
        "\"\"\"\n",
        "Self-attention with causal masking\n",
        "We compute self-attention as usual, but prevent any information to flow from future \n",
        "tokens by masking the upper half of the scaled dot product matrix.\n",
        "\"\"\"\n",
        "class MultiHeadSelfAttention(layers.Layer):  # Attention over the input sentence bottom left\n",
        "\n",
        "  def __init__(self, embed_dim, num_heads=8): # 8 is the parallel attention layers\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    if embed_dim % num_heads != 0:\n",
        "      raise ValueError(f\"ebmedding dimension = {embed_dim} should be divisible by the number of heads = {num_heads}\")\n",
        "  \n",
        "    self.projection_dim = embed_dim // num_heads #floored division\n",
        "    self.query_dense = layers.Dense(embed_dim)\n",
        "    self.key_dense = layers.Dense(embed_dim)\n",
        "    self.value_dense = layers.Dense(embed_dim)\n",
        "    self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "  @staticmethod # Method does not utilize self\n",
        "  # Right side Masked Multi-Head Attention\n",
        "  def causal_attention_mask(n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    return tf.cast(m, dtype)\n",
        "\n",
        "  def attention(self, query, key, value):\n",
        "\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention Sequence\n",
        "    Q, K --> MatMul (Q, K) --> Scale --> Mask --> Softmax --> Matmul (Weights, V)\n",
        "    \"\"\"\n",
        "\n",
        "    # Scaled Dot-Product Attention in the Multi-Head Attention\n",
        "    score = tf.matmul(query, key, transpose_b=True) # Matrix Multiplcation for matrix a (query) and b (key)\n",
        "    dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # Changes the dtype to a float32\n",
        "\n",
        "    # Scale\n",
        "    scaled_score = score / tf.math.sqrt(dim_key) # Computes element-wise square root of the input tensor.\n",
        "\n",
        "    # Mask section in Scaled-Dot Product Attention\n",
        "    shape = tf.shape(scaled_score) # Shape of tensor\n",
        "    dim_dest, dim_src = shape[2], shape[3]\n",
        "    attention_mask = self.causal_attention_mask(dim_dest, dim_src, scaled_score.dtype)\n",
        "    attention_mask = tf.reshape(attention_mask, [1, 1, dim_dest, dim_src])\n",
        "    scaled_score = scaled_score * attention_mask - 1e4 * (1 - attention_mask)\n",
        "\n",
        "    # Computes softmax activation\n",
        "    weights = tf.nn.softmax(scaled_score, axis=-1) \n",
        "\n",
        "    # Finishing Scaled-Dot Product Attention\n",
        "    output = tf.matmul(weights, value)\n",
        "\n",
        "    return output, weights\n",
        "\n",
        "  def separate_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    batch_size = tf.shape(inputs)[0] \n",
        "    query = self.query_dense(inputs) # (batch_size, seq_len, embed_dim)\n",
        "    key = self.key_dense(inputs)\n",
        "    value = self.value_dense(inputs)\n",
        "    query = self.separate_heads(\n",
        "        query, batch_size\n",
        "    )\n",
        "    key = self.separate_heads(\n",
        "        key, batch_size\n",
        "    )\n",
        "    value = self.separate_heads(\n",
        "        value, batch_size\n",
        "    )\n",
        "    attention, weights = self.attention(query, key, value)\n",
        "    attention = tf.transpose(\n",
        "        attention, perm=[0, 2, 1, 3]\n",
        "    )\n",
        "    concat_attention = tf.reshape(\n",
        "        attention, (batch_size, -1, self.embed_dim)\n",
        "    )\n",
        "    output = self.combine_heads(concat_attention)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHSgLj_5TU5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement a Transformer block as a\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "    self.ffn = keras.Sequential(\n",
        "        [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),] # Feedforward Neural Network\n",
        "    )\n",
        "    #Normalize the activations of the previous layer for each given example in a batch independently, \n",
        "    #rather than across a batch like Batch Normalization.\n",
        "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = layers.Dropout(rate)\n",
        "    self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    attention_output = self.att(inputs)\n",
        "    attention_output = self.dropout1(attention_output)\n",
        "    out1 = self.layernorm1(inputs + attention_output)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "\n",
        "    return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0CzSHF1Z-jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement Embedding Layer\n",
        "# Two layers --> one for tokens, one for token index\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "banqpD2pceZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement miniature GPT model\n",
        "\n",
        "vocab_size = 20_000 # The top 20k words\n",
        "maxlen = 100 # Max sequence length\n",
        "embed_dim = 256 # Embedding size for each token\n",
        "num_heads = 2 # Number of attention heads\n",
        "feed_forward_dim = 256 # Hidden layer size in feed forward neural network in transformer\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\", loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnlpwhGDeB22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0765bfba-3f6a-48f8-eaba-5344a3463d00"
      },
      "source": [
        "# Bring in data\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  10.1M      0  0:00:07  0:00:07 --:--:-- 18.0M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXYKa-9neg0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0569f81-33e0-4bb7-bd4a-8c9726358e28"
      },
      "source": [
        "# This will be my data\n",
        "batch_size = 32\n",
        "\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiPlmsxjeq5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataset from these text files\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  # Remove html line-break tags, and handle puncutation\n",
        "  lowercased = tf.strings.lower(input_string)\n",
        "  stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "\n",
        "  return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "# Create vectorization layer and adapy it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size -1,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=maxlen + 1, \n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary() # Get words back from token index\n",
        "\n",
        "def prepare_lm_input_labels(text):\n",
        "  \"\"\"\n",
        "  Shift word sequences by 1 position so that the target for position (i) is\n",
        "  word at position (i+1). The model will use all words up till position (i)\n",
        "  to predict the next word.\n",
        "  \"\"\"\n",
        "  text = tf.expand_dims(text, -1) # Returns a tensor with a length 1 axis inserted at index axis\n",
        "  tokenized_sentences = vectorize_layer(text)\n",
        "  x = tokenized_sentences[:, :-1]\n",
        "  y = tokenized_sentences[:, 1:]\n",
        "  return x, y\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_input_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uf1fnv6e9zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback for generating text\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "  \"\"\"\n",
        "  Callback to generate text from trained model!\n",
        "  1. Feed some starting prompt to the model (tweet)\n",
        "  2. Predict probabilites for next token\n",
        "  3. Sample next token and add it to the next input\n",
        "\n",
        "  # Arguments\n",
        "    max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "    start_tokens: List of integers, the token indices for the starting prompt.\n",
        "    index_to_word: List of strings, obtained from TextVectorization layer.\n",
        "    top_k: Integer, sample from the `top_k` token predictions.\n",
        "    print_every: Integer, print after this many epochs.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "    self.max_tokens = max_tokens\n",
        "    self.start_tokens = start_tokens\n",
        "    self.index_to_word = index_to_word\n",
        "    self.print_every = print_every\n",
        "    self.k = top_k\n",
        "\n",
        "  def sample_from(self, logits):\n",
        "  #Finds values and indices of the k largest entries for the last dimension\n",
        "    logits, indices = tf.math.top_k(logits, k=self.k, sorted=True,) \n",
        "\n",
        "    indices - np.asarray(indices).astype(\"int32\")\n",
        "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "    preds = np.asarray(preds).astype(\"float32\")\n",
        "    return np.random.choice(indices, p=preds)\n",
        "\n",
        "  def detokenize(self, number):\n",
        "    return self.index_to_word[number]\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    start_tokens = [_ for _ in self.start_tokens]\n",
        "    if (epoch +1) % self.print_every !=0:\n",
        "      return\n",
        "\n",
        "    num_tokens_generated = 0\n",
        "    tokens_generated = []\n",
        "\n",
        "    while num_tokens_generated <= self.max_tokens:\n",
        "      pad_len = maxlen - len(start_tokens)\n",
        "      sample_index = len(start_tokens) - 1\n",
        "\n",
        "      if pad_len < 0:\n",
        "        x = start_tokens[:maxlen]\n",
        "        sample_index = maxlen - 1\n",
        "\n",
        "      elif pad_len > 0:\n",
        "        x = start_tokens + [0] * pad_len\n",
        "\n",
        "      else:\n",
        "        x = start_tokens\n",
        "\n",
        "      x = np.array([x])\n",
        "      y, _ = self.model.predict(x)\n",
        "      sample_token = self.sample_from(y[0][sample_index])\n",
        "      tokens_generated.append(sample_token)\n",
        "      start_tokens.append(sample_token)\n",
        "      num_tokens_generated = len(tokens_generated)\n",
        "\n",
        "    # THIS WILL BE THE RESPONSE TO THE TWEET\n",
        "    txt = \" \".join(\n",
        "        [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "    )\n",
        "    print(f\"generated text:\\n{txt}\\n\")\n",
        "    response = pd.DataFrame({'response': [txt]})\n",
        "    response.to_csv('response', index=False)\n",
        "\n",
        "    return txt\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "  word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"this movie is  \"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 20\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siiJ0mVcj4ZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "295273b3-883b-474e-9d81-3da350529739"
      },
      "source": [
        "# Train the Model!\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model.fit(text_ds, verbose=2, epochs=2, callbacks=[text_gen_callback])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "generated text:\n",
            "this movie is a bad movie . it doesn 't really have a great film but the story is great . this movie is\n",
            "\n",
            "1563/1563 - 75s - loss: 5.0633 - dense_20_loss: 5.0633\n",
            "Epoch 2/2\n",
            "generated text:\n",
            "this movie is not a good movie . the story is very well acted , and it is very well done . the story\n",
            "\n",
            "1563/1563 - 74s - loss: 4.4806 - dense_20_loss: 4.4806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f311d7e3c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLp0kIkXEfQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = pd.read_csv('/content/response')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Ei_I-hNiiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "08f34b6b-121f-449b-f46c-6954a810287d"
      },
      "source": [
        "response['response']"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    this movie is not a good movie . the story is ...\n",
              "Name: response, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHNwvypZ2hpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}