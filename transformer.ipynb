{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOM4bv+4J4jM4/UoglJN2bg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7yEFBhfzzop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44fbfeef-f6f0-4928-a70c-14efc6bae144"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMw9mUyS9_rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code inspired and adapted from https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnxTaijq-V97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://arxiv.org/abs/1706.03762\n",
        "\n",
        "\"\"\"\n",
        "Self-attention with causal masking\n",
        "We compute self-attention as usual, but prevent any information to flow from future \n",
        "tokens by masking the upper half of the scaled dot product matrix.\n",
        "\"\"\"\n",
        "class MultiHeadSelfAttention(layers.Layer):  # Attention over the input sentence bottom left\n",
        "\n",
        "  def __init__(self, embed_dim, num_heads=8): # 8 is the parallel attention layers\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    if embed_dim % num_heads != 0:\n",
        "      raise ValueError(f\"ebmedding dimension = {embed_dim} should be divisible by the number of heads = {num_heads}\")\n",
        "  \n",
        "    self.projection_dim = embed_dim // num_heads #floored division\n",
        "    self.query_dense = layers.Dense(embed_dim)\n",
        "    self.key_dense = layers.Dense(embed_dim)\n",
        "    self.value_dense = layers.Dense(embed_dim)\n",
        "    self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "  @staticmethod # Method does not utilize self\n",
        "  # Right side Masked Multi-Head Attention\n",
        "  def causal_attention_mask(n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    return tf.cast(m, dtype)\n",
        "\n",
        "  def attention(self, query, key, value):\n",
        "\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention Sequence\n",
        "    Q, K --> MatMul (Q, K) --> Scale --> Mask --> Softmax --> Matmul (Weights, V)\n",
        "    \"\"\"\n",
        "\n",
        "    # Scaled Dot-Product Attention in the Multi-Head Attention\n",
        "    score = tf.matmul(query, key, transpose_b=True) # Matrix Multiplcation for matrix a (query) and b (key)\n",
        "    dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # Changes the dtype to a float32\n",
        "\n",
        "    # Scale\n",
        "    scaled_score = score / tf.math.sqrt(dim_key) # Computes element-wise square root of the input tensor.\n",
        "\n",
        "    # Mask section in Scaled-Dot Product Attention\n",
        "    shape = tf.shape(scaled_score) # Shape of tensor\n",
        "    dim_dest, dim_src = shape[2], shape[3]\n",
        "    attention_mask = self.causal_attention_mask(dim_dest, dim_src, scaled_score.dtype)\n",
        "    attention_mask = tf.reshape(attention_mask, [1, 1, dim_dest, dim_src])\n",
        "    scaled_score = scaled_score * attention_mask - 1e4 * (1 - attention_mask)\n",
        "\n",
        "    # Computes softmax activation\n",
        "    weights = tf.nn.softmax(scaled_score, axis=-1) \n",
        "\n",
        "    # Finishing Scaled-Dot Product Attention\n",
        "    output = tf.matmul(weights, value)\n",
        "\n",
        "    return output, weights\n",
        "\n",
        "  def separate_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    batch_size = tf.shape(inputs)[0] \n",
        "    query = self.query_dense(inputs) # (batch_size, seq_len, embed_dim)\n",
        "    key = self.key_dense(inputs)\n",
        "    value = self.value_dense(inputs)\n",
        "    query = self.separate_heads(\n",
        "        query, batch_size\n",
        "    )\n",
        "    key = self.separate_heads(\n",
        "        key, batch_size\n",
        "    )\n",
        "    value = self.separate_heads(\n",
        "        value, batch_size\n",
        "    )\n",
        "    attention, weights = self.attention(query, key, value)\n",
        "    attention = tf.transpose(\n",
        "        attention, perm=[0, 2, 1, 3]\n",
        "    )\n",
        "    concat_attention = tf.reshape(\n",
        "        attention, (batch_size, -1, self.embed_dim)\n",
        "    )\n",
        "    output = self.combine_heads(concat_attention)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHSgLj_5TU5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement a Transformer block as a\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "    self.ffn = keras.Sequential(\n",
        "        [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),] # Feedforward Neural Network\n",
        "    )\n",
        "    #Normalize the activations of the previous layer for each given example in a batch independently, \n",
        "    #rather than across a batch like Batch Normalization.\n",
        "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = layers.Dropout(rate)\n",
        "    self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    attention_output = self.att(inputs)\n",
        "    attention_output = self.dropout1(attention_output)\n",
        "    out1 = self.layernorm1(inputs + attention_output)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "\n",
        "    return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0CzSHF1Z-jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement Embedding Layer\n",
        "# Two layers --> one for tokens, one for token index\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "banqpD2pceZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement miniature GPT model\n",
        "\n",
        "vocab_size = 20_000 # The top 20k words\n",
        "maxlen = 100 # Max sequence length\n",
        "embed_dim = 256 # Embedding size for each token\n",
        "num_heads = 2 # Number of attention heads\n",
        "feed_forward_dim = 256 # Hidden layer size in feed forward neural network in transformer\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\", loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnlpwhGDeB22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0765bfba-3f6a-48f8-eaba-5344a3463d00"
      },
      "source": [
        "# Bring in data\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  10.1M      0  0:00:07  0:00:07 --:--:-- 18.0M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXYKa-9neg0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0569f81-33e0-4bb7-bd4a-8c9726358e28"
      },
      "source": [
        "# This will be my data\n",
        "batch_size = 32\n",
        "\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiPlmsxjeq5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataset from these text files\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  # Remove html line-break tags, and handle puncutation\n",
        "  lowercased = tf.strings.lower(input_string)\n",
        "  stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "\n",
        "  return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "# Create vectorization layer and adapy it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size -1,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=maxlen + 1, \n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary() # Get words back from token index\n",
        "\n",
        "def prepare_lm_input_labels(text):\n",
        "  \"\"\"\n",
        "  Shift word sequences by 1 position so that the target for position (i) is\n",
        "  word at position (i+1). The model will use all words up till position (i)\n",
        "  to predict the next word.\n",
        "  \"\"\"\n",
        "  text = tf.expand_dims(text, -1) # Returns a tensor with a length 1 axis inserted at index axis\n",
        "  tokenized_sentences = vectorize_layer(text)\n",
        "  x = tokenized_sentences[:, :-1]\n",
        "  y = tokenized_sentences[:, 1:]\n",
        "  return x, y\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_input_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uf1fnv6e9zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback for generating text\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "  \"\"\"\n",
        "  Callback to generate text from trained model!\n",
        "  1. Feed some starting prompt to the model (tweet)\n",
        "  2. Predict probabilites for next token\n",
        "  3. Sample next token and add it to the next input\n",
        "\n",
        "  # Arguments\n",
        "    max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "    start_tokens: List of integers, the token indices for the starting prompt.\n",
        "    index_to_word: List of strings, obtained from TextVectorization layer.\n",
        "    top_k: Integer, sample from the `top_k` token predictions.\n",
        "    print_every: Integer, print after this many epochs.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "    self.max_tokens = max_tokens\n",
        "    self.start_tokens = start_tokens\n",
        "    self.index_to_word = index_to_word\n",
        "    self.print_every = print_every\n",
        "    self.k = top_k\n",
        "\n",
        "  def sample_from(self, logits):\n",
        "  #Finds values and indices of the k largest entries for the last dimension\n",
        "    logits, indices = tf.math.top_k(logits, k=self.k, sorted=True,) \n",
        "\n",
        "    indices - np.asarray(indices).astype(\"int32\")\n",
        "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "    preds = np.asarray(preds).astype(\"float32\")\n",
        "    return np.random.choice(indices, p=preds)\n",
        "\n",
        "  def detokenize(self, number):\n",
        "    return self.index_to_word[number]\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    start_tokens = [_ for _ in self.start_tokens]\n",
        "    if (epoch +1) % self.print_every !=0:\n",
        "      return\n",
        "\n",
        "    num_tokens_generated = 0\n",
        "    tokens_generated = []\n",
        "\n",
        "    while num_tokens_generated <= self.max_tokens:\n",
        "      pad_len = maxlen - len(start_tokens)\n",
        "      sample_index = len(start_tokens) - 1\n",
        "\n",
        "      if pad_len < 0:\n",
        "        x = start_tokens[:maxlen]\n",
        "        sample_index = maxlen - 1\n",
        "\n",
        "      elif pad_len > 0:\n",
        "        x = start_tokens + [0] * pad_len\n",
        "\n",
        "      else:\n",
        "        x = start_tokens\n",
        "\n",
        "      x = np.array([x])\n",
        "      y, _ = self.model.predict(x)\n",
        "      sample_token = self.sample_from(y[0][sample_index])\n",
        "      tokens_generated.append(sample_token)\n",
        "      start_tokens.append(sample_token)\n",
        "      num_tokens_generated = len(tokens_generated)\n",
        "\n",
        "    # THIS WILL BE THE RESPONSE TO THE TWEET\n",
        "    txt = \" \".join(\n",
        "        [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "    )\n",
        "    print(f\"generated text:\\n{txt}\\n\")\n",
        "    response = pd.DataFrame({'response': [txt]})\n",
        "    response.to_csv('response', index=False)\n",
        "\n",
        "    return txt\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QIzyqM43aZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will be the place to feed in live tweets\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "  word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"this movie is \"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 20\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siiJ0mVcj4ZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1e1b2f3-3176-40e1-c98d-ded1cb136dae"
      },
      "source": [
        "# Train the Model!\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model.fit(text_ds, verbose=2, epochs=30, callbacks=[text_gen_callback])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "generated text:\n",
            "this movie is a great film , i 've ever seen . the plot line is pretty much of the worst movies i have\n",
            "\n",
            "1563/1563 - 76s - loss: 5.0564 - dense_55_loss: 5.0564\n",
            "Epoch 2/30\n",
            "generated text:\n",
            "this movie is about a woman who is a man who is killed by his mother . she is a [UNK] . her daughter\n",
            "\n",
            "1563/1563 - 74s - loss: 4.4888 - dense_55_loss: 4.4888\n",
            "Epoch 3/30\n",
            "generated text:\n",
            "this movie is a shame that it doesn 't seem like it . there 's a lot of people don 't think that 's\n",
            "\n",
            "1563/1563 - 74s - loss: 4.2904 - dense_55_loss: 4.2904\n",
            "Epoch 4/30\n",
            "generated text:\n",
            "this movie is about a man named sam mraovich in the book , who wrote the screenplay , has directed this movie starring a\n",
            "\n",
            "1563/1563 - 76s - loss: 4.1565 - dense_55_loss: 4.1565\n",
            "Epoch 5/30\n",
            "generated text:\n",
            "this movie is about a girl who is in a movie with a young boy . she lives in a way she 's a\n",
            "\n",
            "1563/1563 - 76s - loss: 4.0531 - dense_55_loss: 4.0531\n",
            "Epoch 6/30\n",
            "generated text:\n",
            "this movie is just a little gem . a very good movie , but it is really hard to imagine anyone watching it .\n",
            "\n",
            "1563/1563 - 75s - loss: 3.9690 - dense_55_loss: 3.9690\n",
            "Epoch 7/30\n",
            "generated text:\n",
            "this movie is a bad attempt . it succeeds in the [UNK] of it . i was hoping it should take place at least\n",
            "\n",
            "1563/1563 - 76s - loss: 3.8982 - dense_55_loss: 3.8982\n",
            "Epoch 8/30\n",
            "generated text:\n",
            "this movie is one of the better movies . but it 's so bad it is a good movie . there are a good\n",
            "\n",
            "1563/1563 - 75s - loss: 3.8377 - dense_55_loss: 3.8377\n",
            "Epoch 9/30\n",
            "generated text:\n",
            "this movie is the kind and good to see it . the only good thing about this movie is that it 's a movie\n",
            "\n",
            "1563/1563 - 75s - loss: 3.7848 - dense_55_loss: 3.7848\n",
            "Epoch 10/30\n",
            "generated text:\n",
            "this movie is a little more than a [UNK] film . the movie is so bad . and if you are a little of\n",
            "\n",
            "1563/1563 - 76s - loss: 3.7377 - dense_55_loss: 3.7377\n",
            "Epoch 11/30\n",
            "generated text:\n",
            "this movie is one of the best bollywood movies of the decade . if you have read a lot of the other reviews ,\n",
            "\n",
            "1563/1563 - 76s - loss: 3.6959 - dense_55_loss: 3.6959\n",
            "Epoch 12/30\n",
            "generated text:\n",
            "this movie is a great movie and it is really amazing . it is not very good , just bad . it 's so\n",
            "\n",
            "1563/1563 - 75s - loss: 3.6585 - dense_55_loss: 3.6585\n",
            "Epoch 13/30\n",
            "generated text:\n",
            "this movie is a great , well done movie for kids . . well , if you don 't see a movie , it\n",
            "\n",
            "1563/1563 - 76s - loss: 3.6250 - dense_55_loss: 3.6250\n",
            "Epoch 14/30\n",
            "generated text:\n",
            "this movie is so bad it is bad . it 's bad . . it 's not good . this movie is not good\n",
            "\n",
            "1563/1563 - 77s - loss: 3.5935 - dense_55_loss: 3.5935\n",
            "Epoch 15/30\n",
            "generated text:\n",
            "this movie is about xavier the best friend of the movie ever since . the music is not only good , the movie has\n",
            "\n",
            "1563/1563 - 75s - loss: 3.5649 - dense_55_loss: 3.5649\n",
            "Epoch 16/30\n",
            "generated text:\n",
            "this movie is really a really funny movie . i mean , it is a really cool flick . it doesn 't get the\n",
            "\n",
            "1563/1563 - 74s - loss: 3.5391 - dense_55_loss: 3.5391\n",
            "Epoch 17/30\n",
            "generated text:\n",
            "this movie is not only the acting . it was not even good , but it wasn 't very good either , either way\n",
            "\n",
            "1563/1563 - 75s - loss: 3.5143 - dense_55_loss: 3.5143\n",
            "Epoch 18/30\n",
            "generated text:\n",
            "this movie is not so bad , i 'm not really surprised at imdb .com but this movie is one of the most important\n",
            "\n",
            "1563/1563 - 75s - loss: 3.4921 - dense_55_loss: 3.4921\n",
            "Epoch 19/30\n",
            "generated text:\n",
            "this movie is so bad it 's really not really the worst movie ever made . i can 't believe that a movie that\n",
            "\n",
            "1563/1563 - 75s - loss: 3.4707 - dense_55_loss: 3.4707\n",
            "Epoch 20/30\n",
            "generated text:\n",
            "this movie is not really a waste of time . it was not really boring . the plot was bad , but a good\n",
            "\n",
            "1563/1563 - 76s - loss: 3.4511 - dense_55_loss: 3.4511\n",
            "Epoch 21/30\n",
            "generated text:\n",
            "this movie is a little boring to watch . i 've seen this movie so the worst movie ever and is the plot line\n",
            "\n",
            "1563/1563 - 76s - loss: 3.4329 - dense_55_loss: 3.4329\n",
            "Epoch 22/30\n",
            "generated text:\n",
            "this movie is a little more . the story line says that the plot is simple , and a good story . it 's\n",
            "\n",
            "1563/1563 - 76s - loss: 3.4158 - dense_55_loss: 3.4158\n",
            "Epoch 23/30\n",
            "generated text:\n",
            "this movie is a very interesting and touching movie . the movie starts very well and the ending is pretty funny . i just\n",
            "\n",
            "1563/1563 - 76s - loss: 3.3988 - dense_55_loss: 3.3988\n",
            "Epoch 24/30\n",
            "generated text:\n",
            "this movie is a [UNK] of a man in a mirror . the man is born when he is in [UNK] . he doesn\n",
            "\n",
            "1563/1563 - 76s - loss: 3.3839 - dense_55_loss: 3.3839\n",
            "Epoch 25/30\n",
            "generated text:\n",
            "this movie is very well made in 2003 and hence the name [UNK] \" i saw in the first film . the movie wasn\n",
            "\n",
            "1563/1563 - 75s - loss: 3.3690 - dense_55_loss: 3.3690\n",
            "Epoch 26/30\n",
            "generated text:\n",
            "this movie is about a young man who is trying to get the money . but it has no one knows how to be\n",
            "\n",
            "1563/1563 - 74s - loss: 3.3557 - dense_55_loss: 3.3557\n",
            "Epoch 27/30\n",
            "generated text:\n",
            "this movie is one of the best i 've ever seen . this is one of those films where it is going ? it\n",
            "\n",
            "1563/1563 - 74s - loss: 3.3423 - dense_55_loss: 3.3423\n",
            "Epoch 28/30\n",
            "generated text:\n",
            "this movie is about a man 's girlfriend ! it 's a beautiful movie and a great cast that includes some [UNK] of his\n",
            "\n",
            "1563/1563 - 73s - loss: 3.3299 - dense_55_loss: 3.3299\n",
            "Epoch 29/30\n",
            "generated text:\n",
            "this movie is the best movie i have ever seen ! it 's like watching it and then , i 've finally seen a\n",
            "\n",
            "1563/1563 - 73s - loss: 3.3182 - dense_55_loss: 3.3182\n",
            "Epoch 30/30\n",
            "generated text:\n",
            "this movie is a wonderful little gem . the acting is excellent . it is so feather and cap it down a crush on\n",
            "\n",
            "1563/1563 - 73s - loss: 3.3067 - dense_55_loss: 3.3067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f30a8636a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLp0kIkXEfQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = pd.read_csv('/content/response')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Ei_I-hNiiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "37843cbb-2134-4b00-beff-1c2e7e420fb6"
      },
      "source": [
        "response"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this movie is very entertaining . the acting i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            response\n",
              "0  this movie is very entertaining . the acting i..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg0oVgp0WzvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "41eb00ea-3f6f-4414-c362-d0c8dfce7349"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Capstone\n",
        "%ls"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Capstone\n",
            "charrnn.ipynb  LICENSE.txt           showerthoughts.txt\n",
            "gpt-2.ipynb    showerthoughts_3.csv  transformer.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHNwvypZ2hpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "9fb11a2b-1975-4bf3-e513-53d7856bf0fc"
      },
      "source": [
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vjzm7wc4gRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a11317ed-b2dc-4eb6-bd49-07c60572b2dd"
      },
      "source": [
        "# my_model directory\n",
        "!ls saved_model\n",
        "\n",
        "# Contains an assets folder, saved_model.pb and variables folder.\n",
        "!ls saved_model/my_model"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_model\n",
            "assets\tsaved_model.pb\tvariables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At9oJlfpWEAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_model = tf.keras.models.load_model('saved_model/my_model')"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khhPcjdUWTtw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "7570aa37-4eef-44f7-b42f-e43f0777ce51"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 100, 256)          5145600   \n",
            "_________________________________________________________________\n",
            "transformer_block_7 (Transfo (None, 100, 256)          395776    \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 100, 20000)        5140000   \n",
            "=================================================================\n",
            "Total params: 10,681,376\n",
            "Trainable params: 10,681,376\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeWn6qbHbY1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}